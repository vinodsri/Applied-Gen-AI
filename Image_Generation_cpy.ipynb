{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinodsri/Applied-Gen-AI/blob/main/Image_Generation_cpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests pillow matplotlib huggingface_hub\n",
        "\n",
        "import requests\n",
        "import io\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "\n",
        "# ========================================================\n",
        "# Initialize HF Inference Client\n",
        "# ========================================================\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"üîê Login to Hugging Face\")\n",
        "print(\"Get token from: https://huggingface.co/settings/tokens\")\n",
        "login()\n",
        "\n",
        "# Initialize client\n",
        "client = InferenceClient()\n",
        "print(\"‚úÖ Client ready!\")\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# ========================================================\n",
        "# Helper Functions\n",
        "# ========================================================\n",
        "def show_images(images, titles, size=(15, 5)):\n",
        "    \"\"\"Display images side by side\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=size)\n",
        "    if len(images) == 1:\n",
        "        axes = [axes]\n",
        "    for img, title, ax in zip(images, titles, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title, fontsize=11)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def load_image_url(url):\n",
        "    \"\"\"Load image from URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=20)\n",
        "        return Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ========================================================\n",
        "# Text-to-Image Function\n",
        "# ========================================================\n",
        "def generate_image(prompt, negative_prompt=\"\", model=\"stabilityai/stable-diffusion-xl-base-1.0\"):\n",
        "    \"\"\"Generate image from text using HF Inference API\"\"\"\n",
        "    print(f\"üé® Generating with: {model.split('/')[-1]}\")\n",
        "    print(f\"üìù Prompt: {prompt[:70]}...\")\n",
        "    print(\"‚è≥ Processing (may take 30-60 seconds)...\")\n",
        "\n",
        "    try:\n",
        "        image = client.text_to_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            model=model,\n",
        "            guidance_scale=7.5,\n",
        "            num_inference_steps=30\n",
        "        )\n",
        "        print(\"‚úÖ Done!\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        print(\"\\nüí° Tip: Free tier has rate limits. Wait a minute and try again.\")\n",
        "        return None\n",
        "\n",
        "# ========================================================\n",
        "# Image-to-Image Function\n",
        "# ========================================================\n",
        "def modify_image(init_image, prompt, negative_prompt=\"\", strength=0.55, model=\"stabilityai/stable-diffusion-xl-base-1.0\"):\n",
        "    \"\"\"Modify existing image based on prompt\"\"\"\n",
        "    print(f\"üé® Modifying with: {model.split('/')[-1]}\")\n",
        "    print(f\"üìù Prompt: {prompt[:70]}...\")\n",
        "    print(f\"üí™ Strength: {strength}\")\n",
        "    print(\"‚è≥ Processing (may take 30-45 seconds)...\")\n",
        "\n",
        "    try:\n",
        "        # Note: HF Inference API for img2img might have limitations\n",
        "        # This is a workaround using text-to-image with strong guidance\n",
        "        result = client.text_to_image(\n",
        "            prompt=f\"{prompt}, based on reference image\",\n",
        "            negative_prompt=negative_prompt,\n",
        "            model=model,\n",
        "            guidance_scale=7.5 + (strength * 5),  # Adjust guidance based on strength\n",
        "            num_inference_steps=int(30 * (1 - strength * 0.5))\n",
        "        )\n",
        "        print(\"‚úÖ Done!\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ========================================================\n",
        "# Example 1 - Text to Image\n",
        "# ========================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"EXAMPLE 1: TEXT-TO-IMAGE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "prompt = \"ultra detailed portrait of an explorer in a sunlit library, warm light, shallow depth of field, cinematic, 85mm photo\"\n",
        "negative = \"blurry, low quality, jpeg artifacts, text, watermark, disfigured\"\n",
        "\n",
        "image = generate_image(prompt, negative)\n",
        "\n",
        "if image:\n",
        "    image.save(\"outputs/hf_text2img.png\")\n",
        "    print(\"üíæ Saved: outputs/hf_text2img.png\")\n",
        "    show_images([image], [\"Generated Image\"])\n",
        "\n",
        "# ========================================================\n",
        "#  Example 2 - Different Styles\n",
        "# ========================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXAMPLE 2: DIFFERENT ARTISTIC STYLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "styles = [\n",
        "    (\"watercolor painting, soft colors, artistic\", \"Watercolor\"),\n",
        "    (\"oil painting, impasto technique, rich textures\", \"Oil Painting\"),\n",
        "    (\"digital art, concept art, highly detailed\", \"Digital Art\"),\n",
        "]\n",
        "\n",
        "base_prompt = \"a mountain landscape at sunset\"\n",
        "negative = \"blurry, low quality, text, watermark\"\n",
        "\n",
        "results = []\n",
        "titles = []\n",
        "\n",
        "for style_desc, style_name in styles:\n",
        "    print(f\"\\nüé® Creating {style_name} style...\")\n",
        "    full_prompt = f\"{base_prompt}, {style_desc}\"\n",
        "    img = generate_image(full_prompt, negative)\n",
        "\n",
        "    if img:\n",
        "        results.append(img)\n",
        "        titles.append(style_name)\n",
        "        img.save(f\"outputs/style_{style_name.lower().replace(' ', '_')}.png\")\n",
        "\n",
        "if results:\n",
        "    show_images(results, titles, size=(18, 6))\n",
        "\n",
        "# ========================================================\n",
        "# Example 3 - Using Alternative Models\n",
        "# ========================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXAMPLE 3: TRYING DIFFERENT MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try different available models\n",
        "models_to_try = [\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "]\n",
        "\n",
        "prompt = \"a cozy coffee shop interior, warm lighting, photorealistic\"\n",
        "negative = \"blurry, low quality\"\n",
        "\n",
        "results = []\n",
        "titles = []\n",
        "\n",
        "for model in models_to_try:\n",
        "    print(f\"\\nü§ñ Testing: {model.split('/')[-1]}\")\n",
        "    img = generate_image(prompt, negative, model=model)\n",
        "\n",
        "    if img:\n",
        "        results.append(img)\n",
        "        titles.append(model.split('/')[-1])\n",
        "\n",
        "    # Add delay to avoid rate limits\n",
        "    import time\n",
        "    time.sleep(5)\n",
        "\n",
        "if results:\n",
        "    show_images(results, titles, size=(12, 6))\n",
        "\n",
        "# ========================================================\n",
        "# Tips and Information\n",
        "# ========================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìö USING HUGGING FACE INFERENCE API\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüéâ Demo complete!\")"
      ],
      "metadata": {
        "id": "9CHwiaFk-VDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# LangChain + OpenAI DALL-E Image Generation - Simple Demo\n",
        "# ========================================================\n",
        "\n",
        "# Install Libraries\n",
        "!pip install -q langchain langchain-openai openai pillow requests\n",
        "\n",
        "\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from openai import OpenAI as OpenAIClient\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enter your OpenAI API key\n",
        "OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "client = OpenAIClient(api_key=OPENAI_API_KEY)\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "\n",
        "# Create LangChain Prompt Enhancer\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"description\"],\n",
        "    template=\"Enhance this into a detailed DALL-E prompt: {description}\\n\\nDetailed prompt:\"\n",
        ")\n",
        "\n",
        "llm = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-instruct\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "print(\"‚úÖ LangChain ready!\")\n",
        "\n",
        "#Image Generation Function\n",
        "def generate_image(description, use_langchain=True):\n",
        "    \"\"\"Generate image with optional LangChain enhancement\"\"\"\n",
        "\n",
        "    # Step 1: Enhance prompt with LangChain (optional)\n",
        "    if use_langchain:\n",
        "        print(f\"Original: {description}\")\n",
        "        enhanced = chain.run(description=description)\n",
        "        print(f\"Enhanced: {enhanced}\\n\")\n",
        "        prompt = enhanced\n",
        "    else:\n",
        "        prompt = description\n",
        "\n",
        "    # Step 2: Generate with DALL-E\n",
        "    print(\"Generating image...\")\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    # Step 3: Display image\n",
        "    image_url = response.data[0].url\n",
        "    img_response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(description[:50])\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Done!\\n\")\n",
        "    return img\n",
        "\n",
        "#  Example 1 - Without LangChain\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Direct DALL-E Generation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "generate_image(\"a cat wearing sunglasses\", use_langchain=False)\n",
        "\n",
        "#  Example 2 - With LangChain Enhancement\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 2: With LangChain Enhancement\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "generate_image(\"a futuristic city\", use_langchain=True)\n",
        "\n",
        "#  Create Your Own\n",
        "print(\"=\" * 60)\n",
        "print(\"Your Turn!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "user_input = input(\"Describe your image: \")\n",
        "enhance = input(\"Use LangChain? (yes/no): \").lower() == \"yes\"\n",
        "\n",
        "generate_image(user_input, use_langchain=enhance)"
      ],
      "metadata": {
        "id": "2DpBFiELnTNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "id": "s20oRxSY1vSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}